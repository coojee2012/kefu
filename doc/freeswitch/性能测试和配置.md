# 性能测试和配置

## About
Discussion of testing performance of FreeSWITCH™ with links to test scenario open source projects.

## Measures of Performance

When people say performance it can mean a wide variety of things. In reality performance typically comes down to two bottle necks which are SIP, and RTP. These typically translate into calls per second and concurrent calls respectively. Additionally, high volume systems might experience bottlenecks with database servers running out of connections or even bandwidth when looking up account or configuration data.

当人们说性能可以是各种各样的东西。在现实中表现通常可以归结为两个瓶颈SIP,和RTP。这些通常分别转化为每秒调用和并发调用。此外,高容量系统可能经历瓶颈与数据库服务器的连接,甚至当查找帐户或配置数据带宽。

### Calls per Second (CPS)

Since calls per second is simply a measure of how many calls are being setup and torn down per second the limiting factor is the ability to process the SIP messages. Depending on the type of traffic you have this may or may not be a factor. There are a variety of components that can contribute to this bottleneck, FreeSWITCH and it's libraries being only some of them.
因为每秒调用只是一个衡量有多少电话安装和拆除每秒的限制因素是处理SIP消息的能力。根据流量的类型你有这个可能是也可能不是一个因素。有各种各样的组件,这些组件可以导致这个瓶颈,FreeSWITCH和它的库只是其中的一些。

### Concurrent Calls

Using modern hardware concurrent calls is less a limit of SIP but rather the RTP media streaming. This can further be broken down to available bandwidth and the packets per second. The theoretical limit on concurrent calls through a gigabit Ethernet port would be around 10,500 calls without RTCP, assuming G.711 codec and the link-level overheads. Theory is great and all, but in reality the kernel networking layer will be your limiting factor due to the packets per second of the RTP media stream.
使用现代的硬件并行调用与其说是SIP而是RTP流媒体的限制。这可以进一步被分解可用带宽和每秒数据包。理论限制并发调用通过一个千兆以太网端口将大约10500调用没有服务器,假设G.711编解码器和链路级别的管理费用。理论是伟大的,但在现实中内核网络层将是你限制因素由于每秒数据包的RTP媒体流。

### Multi-threaded

FreeSWITCH uses threading. In modern linux kernels threading and forking are very similar. Normally the 'top' utility only shows 1 FS process because top by default rolls up all the threads into one entry. The command 'top -H' will show individual threads.
FreeSWITCH使用线程。在现代linux内核线程和分叉是非常相似的。通常“top”实用程序默认只显示1 FS过程因为前卷起所有线程在一个条目。命令“top -H”将显示各个线程。

'htop' by default shows you the individual threads.
“htop”默认情况下显示了单个线程。

In FreeSWITCH there are several threads running on just a base idle FreeSWITCH process. Each additional call leg is at least 1 more thread. Depending on which applications are active on a call, there could be more than 1 thread per call leg.

在FreeSWITCH有多个线程上运行空闲FreeSWITCH过程只是一个基地。每个额外的调用腿至少一个线程。活跃在一个叫,这取决于应用程序可能有超过1线程调用的腿。

## Configurations

Recommended Configurations
A 64-bit CPU running a 64-bit operating system and a 64-bit version of FreeSWITCH is recommended. A bare metal system provides consistent, predictable performance and most importantly for real–time applications like this, a reliable kernel clock for RTP packet timing. With a virtual machine it is difficult to determine where any problems might originate and improper propagation of the hardware clock through the VM host to the guest operating system is not always available so the RTP tests will be rendered meaningless.
64位CPU运行一个64位操作系统和64位版本的FreeSWITCH推荐。裸露的金属系统提供一致的、可预测的性能以及对实时应用程序的最重要的是这样的,一个可靠的内核时钟RTP数据包时机。虚拟机很难确定,可能产生的任何问题和不适当的传播通过VM主机硬件时钟的来宾操作系统并不总是可用的RTP测试将变得毫无意义。

Debian linux is the recommended OS, since that's the OS used by the core developers and therefore the best tested. It will work on some other operating systems though.
Debian linux是推荐的操作系统,因为这是使用的操作系统核心开发人员,因此最好的考验。它将工作在其他操作系统上。

## Recommended ULIMIT settings 推荐ULIMIT设置

The following are recommended ulimit settings for FreeSWITCH when you want maximum performance. Ulimit settings you can add to initd script before do_start().
下面是推荐FreeSWITCH ulimit设置当您希望最大的性能。Ulimit设置您可以添加initd脚本之前do_start()。

ulimit -c unlimited # The maximum size of core files created.
ulimit -d unlimited # The maximum size of a process's data segment.进程的数据段的最大大小。
ulimit -f unlimited # The maximum size of files created by the shell (default option)
ulimit -i unlimited # The maximum number of pending signals
ulimit -n 999999    # The maximum number of open file descriptors.
ulimit -q unlimited # The maximum POSIX message queue size
ulimit -u unlimited # The maximum number of processes available to a single user.
ulimit -v unlimited # The maximum amount of virtual memory available to the process.
ulimit -x unlimited # ???
ulimit -s 240         # The maximum stack size
ulimit -l unlimited # The maximum size that may be locked into memory.
ulimit -a           # All current limits are reported.
Ethernet Tuning in linux 在linux中以太网调优

## Beware buffer bloat 当心缓冲膨胀

Prior to the bufferbloat guys coming in and talking to us there was a note in here that one should "set the buffers to maximum." That advice is WRONG on so many levels. To make a long story short, when you're doing real-time media like VoIP you absolutely do not want large buffers. On an unsaturated network link you won't notice anything, but when you have a saturated network the larger buffers will cause your RTP packets to be buffered instead of discarded.
bufferbloat家伙进来,之前跟我们有一个在这里,一个人应该“将缓冲区设置为最大值。”这个建议在很多层面上是错误的。长话短说,当你在做实时媒体VoIP你绝对不希望大的缓冲区。不饱和网络链接你不会注意到任何东西,但是当你有一个饱和网络更大的缓冲区将导致你的RTP数据包缓冲而不是丢弃
So, what should your rx/tx queuelens be? Only you can know for sure, but it's good to experiment. Normally in linux it defaults to 1000. IF you are using a good traffic shaping qdisc (pfifo_fast or SFB or others) AND prioritizing udp/rtp traffic you can leave it alone, but it still is a good idea to lower it significantly for VoIP applications, depending on your workload and connectivity.

Don't use the default pfifo qdisc, regardless. It outputs packets in strict fifo order.

To see your current settings use ethtool:

ethtool settings
[root@server:~]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maxima:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
Current hardware settings:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             128
These were the defaults on my Lenny install. If you needed to change it you can do this:

ethtool suggested changes
[root@server:~]# ethtool -G eth0 rx 128
[root@server:~]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maxima:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
Current hardware settings:
RX:             128
RX Mini:        0
RX Jumbo:       0
TX:             128

There is no one correct answer to what you should set the ring buffers to. It all depends on your traffic. Dave Taht from the Bufferbloat project reports that, based on his observations and experiences and papers such as http://www.cs.clemson.edu/~jmarty/papers/bittorrentBroadnets.pdf , that at present in home systems it is better to have no more than 32 unmanaged TX buffers on a 100Mbit network. It appears on my Lenny they are 32/64:

One man's buffer settings
[root@server:~]# ethtool -G eth0 rx 32 tx 32
[root@server:~]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:             256
RX Mini:        0
RX Jumbo:       0
TX:             256
Current hardware settings:
RX:             32
RX Mini:        0
RX Jumbo:       0
TX:             64
You'll note you can't with this driver reduce the TX buffer to a more optimum level!! This means that you will incur nearly a 10ms delay in the driver alone (at maximum packet size and load) on packets if you are on a 100Mbit network.

(similarly, a large TXQUEUELEN translates to lots of delay too)

On a gigibit network interface, the default TX queues and TXQUEUELEN are closer to usable, but still too large.

Having larger RX buffers is OK, to some extent. You need to be able to absorb bursts without packet loss. Tuning and observation of actual packet loss on the receive channel is a good idea.

And lastly, the optimum for TX is much lower on a 3Mbit uplink than a 100Mbit uplink. The debloat-testing kernel contains some Ethernet and wireless drivers that allow reducing TX to 1.

TCP/IP Tuning
For a server that is used primarily for VoIP, TCP Cubic (the default in Linux) can stress the network subsystem too much. Using TCP Vegas (which ramps up based on measured latency) is used by several FreeSWITCH users in production, as a "kinder, gentler" TCP for command and control functions.

To enable Vegas rather than Cubic you can, at boot:

modprobe tcp_vegas
echo vegas > /proc/sys/net/ipv4/tcp_congestion_control
--- Some interesting comments about tcp_vegas at http://tomatousb.org/forum/t-267882/

FreeSWITCH's core.db I/O Bottleneck
On a normal configuration, core.db is written to disk almost every second, generating hundreds of block-writes per second. To avoid this problem, turn /usr/local/freeswitch/db into an in-memory filesystem. If you use SSDs, it is CRITICAL that you move core.db to a RAM disk to prolong the life of the SSD.

On current FreeSWITCH versions you should use the documented "core-db-name" parameter in switch.conf.xml (simply restart FreeSwitch to apply the changes):

```xml
   <param name="core-db-name" value="/dev/shm/core.db" />
```

Otherwise you may create a dedicated in-memory filesystem, for example by adding the following to the end of /etc/fstab

### fstab Example

Example of /etc/fstab entry (using default size)

`tmpfs /usr/local/freeswitch/db tmpfs defaults 0 0`

To specify a size for the filesystem use the appropriate mount(1) option:

`tmpfs /usr/local/freeswitch/db tmpfs defaults,size=4g 0 0`

To use the new filesystem run the following commands (or the equivalent commands for your OS):

```bash
mount /usr/local/freeswitch/db
/etc/init.d/freeswitch restart
```

An alternative is to move the core DB into an ODBC database, which will move this processing to a DBMS which is capable of handling large numbers of requests far better and can even move this processing onto another server. Consider using freeswitch.dbh to take advantage of pooling.

## Stress Testing 压力测试

KNOW

IF YOU DO NOT UNDERSTAND HOW TO STRESS TEST PROPERLY THEN YOUR RESULTS WILL BE WORTHLESS.
如果你不明白如何正确的压力测试,那么你的效果会一文不值。

Using SIPp is part dark art, part voodoo, part Santeria. YOU HAVE BEEN WARNED
使用SIPp是黑暗艺术,一部分巫术,Santeria一部分。我已经警告过你了

When using SIPp's uas and uac to test FreeSWITCH, you need to make sure there is media back and forth. If you just send media from one sipp to another without echoing the RTP back (-rtp_echo), FS will timeout due to MEDIA_TIMEOUT. This is to avoid incorrect billing when one side has no media for more than certain period of time.

当使用SIPp测试FreeSWITCH无人机和uac,您需要确保有媒体来回。如果你只是从一个sipp媒体发送到另一个没有呼应了RTP(-rtp_echo),由于MEDIA_TIMEOUT FS将超时。这是为了避免不正确的账单当一方没有媒体超过一定的时间。

See Also
FreeSWITCH performance test on PC Engines APU — Stanislav Sinyagin tests FreeSWITCH™ transcoding performance with only one test machine

Real-world observations — Post measurements of your experience at using FreeSWITCH™ in a real-world configuration, not a stress test.

SSD Tuning for Linux — special considerations for systems using Solid State Drives for storage

SIPp — Open source test toll and traffic generator for SIP

SIPpy Cup — Ben Langfeld contributes this scenario generator for SIPp to simplify the creation of test profiles and especially compatible media

check_voip_call — Henry Huang contributes this project to work with Nagios

http://www.bandcalc.com/ — Bandwidth calculator for different codecs and use cases

http://www.cs.clemson.edu/%7Ejmarty/papers/bittorrentBroadnets.pdf — Paper on buffer sizing based on bittorrent usage
